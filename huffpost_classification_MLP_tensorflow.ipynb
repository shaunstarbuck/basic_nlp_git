{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the huffpost article dataset (json file)\n",
    "df =  pd.read_json(\"data/news-category-dataset/News_Category_Dataset_v2.json\", lines =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert labels from string to int for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary of category:int\n",
    "categories = set(df['category'])\n",
    "i = 0\n",
    "category_dict = {}\n",
    "for item in categories:\n",
    "    category_dict[item] = i\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column of category ints\n",
    "df['category_int'] = df['category'].map(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the url into a usable format for text analysis\n",
    "def tokenize_url(url):\n",
    "    #remove huffpost portion\n",
    "    url = url.replace(\"https://www.huffingtonpost.com/entry/\",\"\")\n",
    "    #use regular expression to convert underscore into a space\n",
    "    url = re.sub(\"(\\W|_)+\",\" \",url)\n",
    "    return url\n",
    "\n",
    "df['tokenized_url'] = df['link'].apply(lambda x:tokenize_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine headline and description\n",
    "df['headline_desc'] = df['headline'] + df['short_description']\n",
    "\n",
    "#description + headline + url\n",
    "df['headline_desc_url'] =  df['headline_desc'] + ' ' + df['tokenized_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150639\n",
      "50214\n"
     ]
    }
   ],
   "source": [
    "#Split into training and testing data\n",
    "train_data, test_data = train_test_split(df, random_state = 2000)\n",
    "print(str(len(train_data)) +'\\n' + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to lists of texts,labels and train,test\n",
    "train_texts = list(train_data['headline_desc_url'])\n",
    "train_labels = list(train_data['category_int'])\n",
    "\n",
    "test_texts = list(test_data['headline_desc_url'])\n",
    "test_labels= list(test_data['category_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, n_gram = (1,1), feature_size = 20000):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "        n_gram: tuple, indicating the size of ngrams to use (1,1) is single words\n",
    "        feature_size: int, indicating the number of features to use\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Size of n-grams to use (tuple)\n",
    "    NGRAM_RANGE = n_gram\n",
    "    TOKEN_MODE = 'word'\n",
    "    MIN_DOCUMENT_FREQUENCY = 2\n",
    "    #FEATURE_COUNT =  feature_size\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "            'max_features' : feature_size\n",
    "    }\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    \n",
    "    #Learn the vocabulary from training texts and vectorize them\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    \n",
    "    #Vectorize validation texts\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    #Select top k of the vectorized features\n",
    "    #selector = SelectKBest(f_classif, k=min(feature_size, x_train.shape[1]))\n",
    "    #selector.fit(x_train, train_labels)\n",
    "    #x_train = selector.transform(x_train).astype('float32')\n",
    "    #x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = ngram_vectorize(train_texts, train_labels, test_texts, feature_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units = num_classes\n",
    "    #Use Softmax final activation layer for multiclass\n",
    "    op_activation = 'softmax'\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2,\n",
    "                      n_gram = (1,2),\n",
    "                      feature_size = 20000):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    #num_classes = explore_data.get_num_classes(train_labels)\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    #unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    #if len(unexpected_labels):\n",
    "     #   raise ValueError('Unexpected label values found in the validation set:'\n",
    "      #                   ' {unexpected_labels}. Please make sure that the '\n",
    "       #                  'labels in the validation set are in the same range '\n",
    "        #                 'as training labels.'.format(\n",
    "         #                    unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    #x_train, x_val = #vectorize_data.\n",
    "    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts, n_gram = n_gram, feature_size = feature_size)\n",
    "\n",
    "    # Create model instance.\n",
    "    #model = #build_model.\n",
    "    model = mlp_model(layers=layers,\n",
    "                        units=units,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        input_shape=x_train.shape[1:],\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('huffpost_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (train_texts, train_labels), (test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150639 samples, validate on 50214 samples\n",
      "Epoch 1/1000\n",
      "150639/150639 - 5s - loss: 2.4949 - acc: 0.3789 - val_loss: 1.8730 - val_acc: 0.5160\n",
      "Epoch 2/1000\n",
      "150639/150639 - 6s - loss: 1.9034 - acc: 0.4999 - val_loss: 1.6500 - val_acc: 0.5603\n",
      "Epoch 3/1000\n",
      "150639/150639 - 5s - loss: 1.7809 - acc: 0.5231 - val_loss: 1.5734 - val_acc: 0.5726\n",
      "Epoch 4/1000\n",
      "150639/150639 - 5s - loss: 1.7261 - acc: 0.5338 - val_loss: 1.5389 - val_acc: 0.5811\n",
      "Epoch 5/1000\n",
      "150639/150639 - 5s - loss: 1.6871 - acc: 0.5400 - val_loss: 1.5158 - val_acc: 0.5830\n",
      "Epoch 6/1000\n",
      "150639/150639 - 5s - loss: 1.6650 - acc: 0.5453 - val_loss: 1.5028 - val_acc: 0.5870\n",
      "Epoch 7/1000\n",
      "150639/150639 - 5s - loss: 1.6412 - acc: 0.5488 - val_loss: 1.4893 - val_acc: 0.5909\n",
      "Epoch 8/1000\n",
      "150639/150639 - 5s - loss: 1.6243 - acc: 0.5524 - val_loss: 1.4812 - val_acc: 0.5918\n",
      "Epoch 9/1000\n",
      "150639/150639 - 5s - loss: 1.6111 - acc: 0.5560 - val_loss: 1.4746 - val_acc: 0.5923\n",
      "Epoch 10/1000\n",
      "150639/150639 - 5s - loss: 1.5979 - acc: 0.5575 - val_loss: 1.4687 - val_acc: 0.5942\n",
      "Epoch 11/1000\n",
      "150639/150639 - 5s - loss: 1.5871 - acc: 0.5594 - val_loss: 1.4639 - val_acc: 0.5942\n",
      "Epoch 12/1000\n",
      "150639/150639 - 5s - loss: 1.5762 - acc: 0.5624 - val_loss: 1.4616 - val_acc: 0.5939\n",
      "Epoch 13/1000\n",
      "150639/150639 - 5s - loss: 1.5695 - acc: 0.5631 - val_loss: 1.4557 - val_acc: 0.5967\n",
      "Epoch 14/1000\n",
      "150639/150639 - 5s - loss: 1.5611 - acc: 0.5652 - val_loss: 1.4529 - val_acc: 0.5964\n",
      "Epoch 15/1000\n",
      "150639/150639 - 5s - loss: 1.5506 - acc: 0.5666 - val_loss: 1.4499 - val_acc: 0.5972\n",
      "Epoch 16/1000\n",
      "150639/150639 - 5s - loss: 1.5433 - acc: 0.5699 - val_loss: 1.4495 - val_acc: 0.5983\n",
      "Epoch 17/1000\n",
      "150639/150639 - 5s - loss: 1.5365 - acc: 0.5699 - val_loss: 1.4454 - val_acc: 0.5998\n",
      "Epoch 18/1000\n",
      "150639/150639 - 5s - loss: 1.5362 - acc: 0.5702 - val_loss: 1.4458 - val_acc: 0.5985\n",
      "Epoch 19/1000\n",
      "150639/150639 - 5s - loss: 1.5304 - acc: 0.5709 - val_loss: 1.4454 - val_acc: 0.5990\n",
      "Epoch 20/1000\n",
      "150639/150639 - 5s - loss: 1.5180 - acc: 0.5736 - val_loss: 1.4428 - val_acc: 0.5997\n",
      "Epoch 21/1000\n",
      "150639/150639 - 5s - loss: 1.5224 - acc: 0.5720 - val_loss: 1.4433 - val_acc: 0.5993\n",
      "Epoch 22/1000\n",
      "150639/150639 - 5s - loss: 1.5135 - acc: 0.5749 - val_loss: 1.4423 - val_acc: 0.6001\n",
      "Epoch 23/1000\n",
      "150639/150639 - 5s - loss: 1.5092 - acc: 0.5754 - val_loss: 1.4432 - val_acc: 0.6005\n",
      "Epoch 24/1000\n",
      "150639/150639 - 5s - loss: 1.5039 - acc: 0.5758 - val_loss: 1.4443 - val_acc: 0.5995\n",
      "Validation accuracy: 0.5994941592216492, loss: 1.4442867712815584\n"
     ]
    }
   ],
   "source": [
    "hist = train_ngram_model(data, n_gram = (1,1), feature_size = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 2000 single word ngrams gave a validation accuracy of ~60% (58% training) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6063648, 1.3968539326242322)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150639 samples, validate on 50214 samples\n",
      "Epoch 1/1000\n",
      "150639/150639 - 43s - loss: 2.3558 - acc: 0.4160 - val_loss: 1.6118 - val_acc: 0.5778\n",
      "Epoch 2/1000\n",
      "150639/150639 - 43s - loss: 1.5073 - acc: 0.5978 - val_loss: 1.3082 - val_acc: 0.6377\n",
      "Epoch 3/1000\n",
      "150639/150639 - 43s - loss: 1.2689 - acc: 0.6503 - val_loss: 1.2148 - val_acc: 0.6581\n",
      "Epoch 4/1000\n",
      "150639/150639 - 43s - loss: 1.1321 - acc: 0.6804 - val_loss: 1.1771 - val_acc: 0.6658\n",
      "Epoch 5/1000\n",
      "150639/150639 - 43s - loss: 1.0385 - acc: 0.7024 - val_loss: 1.1620 - val_acc: 0.6678\n",
      "Epoch 6/1000\n",
      "150639/150639 - 43s - loss: 0.9657 - acc: 0.7202 - val_loss: 1.1603 - val_acc: 0.6673\n",
      "Epoch 7/1000\n",
      "150639/150639 - 43s - loss: 0.9037 - acc: 0.7347 - val_loss: 1.1663 - val_acc: 0.6667\n",
      "Epoch 8/1000\n",
      "150639/150639 - 43s - loss: 0.8502 - acc: 0.7480 - val_loss: 1.1773 - val_acc: 0.6657\n",
      "Validation accuracy: 0.6657107472419739, loss: 1.1772757229822275\n"
     ]
    }
   ],
   "source": [
    "hist = train_ngram_model(data, n_gram = (1,1), feature_size = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 20000 single word ngrams gave a validation accuracy of ~67% (75% training) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150639 samples, validate on 50214 samples\n",
      "Epoch 1/1000\n",
      "150639/150639 - 43s - loss: 2.3265 - acc: 0.4262 - val_loss: 1.6045 - val_acc: 0.5811\n",
      "Epoch 2/1000\n",
      "150639/150639 - 43s - loss: 1.4858 - acc: 0.6017 - val_loss: 1.3330 - val_acc: 0.6384\n",
      "Epoch 3/1000\n",
      "150639/150639 - 43s - loss: 1.2418 - acc: 0.6574 - val_loss: 1.2482 - val_acc: 0.6513\n",
      "Epoch 4/1000\n",
      "150639/150639 - 44s - loss: 1.1051 - acc: 0.6880 - val_loss: 1.2158 - val_acc: 0.6549\n",
      "Epoch 5/1000\n",
      "150639/150639 - 45s - loss: 1.0072 - acc: 0.7109 - val_loss: 1.2053 - val_acc: 0.6569\n",
      "Epoch 6/1000\n",
      "150639/150639 - 44s - loss: 0.9243 - acc: 0.7303 - val_loss: 1.2077 - val_acc: 0.6574\n",
      "Epoch 7/1000\n",
      "150639/150639 - 44s - loss: 0.8567 - acc: 0.7455 - val_loss: 1.2160 - val_acc: 0.6562\n",
      "Validation accuracy: 0.6562114357948303, loss: 1.2160426570783547\n"
     ]
    }
   ],
   "source": [
    "hist = train_ngram_model(data, n_gram = (1,2), feature_size = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 20000 single and biword word ngrams gave a validation accuracy of ~66% (75% training) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150639 samples, validate on 50214 samples\n",
      "Epoch 1/1000\n",
      "150639/150639 - 66s - loss: 2.3141 - acc: 0.4311 - val_loss: 1.5879 - val_acc: 0.5834\n",
      "Epoch 2/1000\n",
      "150639/150639 - 65s - loss: 1.4310 - acc: 0.6156 - val_loss: 1.3037 - val_acc: 0.6420\n",
      "Epoch 3/1000\n",
      "150639/150639 - 65s - loss: 1.1544 - acc: 0.6804 - val_loss: 1.2157 - val_acc: 0.6588\n",
      "Epoch 4/1000\n",
      "150639/150639 - 65s - loss: 0.9882 - acc: 0.7193 - val_loss: 1.1888 - val_acc: 0.6618\n",
      "Epoch 5/1000\n",
      "150639/150639 - 65s - loss: 0.8689 - acc: 0.7496 - val_loss: 1.1861 - val_acc: 0.6632\n",
      "Epoch 6/1000\n",
      "150639/150639 - 66s - loss: 0.7714 - acc: 0.7738 - val_loss: 1.2004 - val_acc: 0.6607\n",
      "Epoch 7/1000\n",
      "150639/150639 - 66s - loss: 0.6922 - acc: 0.7941 - val_loss: 1.2172 - val_acc: 0.6591\n",
      "Validation accuracy: 0.6591189503669739, loss: 1.2172032026481652\n"
     ]
    }
   ],
   "source": [
    "hist = train_ngram_model(data, n_gram = (1,2), feature_size = 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 30000 single and biword word ngrams gave a validation accuracy of ~66% (79% training) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use anova F test instead of top tf-idf value to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts, n_gram = (1,1), feature_size = 20000):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "        n_gram: tuple, indicating the size of ngrams to use (1,1) is single words\n",
    "        feature_size: int, indicating the number of features to use\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Size of n-grams to use (tuple)\n",
    "    NGRAM_RANGE = n_gram\n",
    "    TOKEN_MODE = 'word'\n",
    "    MIN_DOCUMENT_FREQUENCY = 2\n",
    "    #FEATURE_COUNT =  feature_size\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "            #'max_features' : feature_size\n",
    "    }\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    \n",
    "    #Learn the vocabulary from training texts and vectorize them\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    \n",
    "    #Vectorize validation texts\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    #Select top k of the vectorized features by ANOVA F-test\n",
    "    selector = SelectKBest(f_classif, k=min(feature_size, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150639 samples, validate on 50214 samples\n",
      "Epoch 1/1000\n",
      "150639/150639 - 31s - loss: 2.4446 - acc: 0.4019 - val_loss: 1.7240 - val_acc: 0.5567\n",
      "Epoch 2/1000\n",
      "150639/150639 - 30s - loss: 1.6085 - acc: 0.5797 - val_loss: 1.3925 - val_acc: 0.6232\n",
      "Epoch 3/1000\n",
      "150639/150639 - 30s - loss: 1.3664 - acc: 0.6321 - val_loss: 1.2736 - val_acc: 0.6481\n",
      "Epoch 4/1000\n",
      "150639/150639 - 30s - loss: 1.2407 - acc: 0.6603 - val_loss: 1.2195 - val_acc: 0.6612\n",
      "Epoch 5/1000\n",
      "150639/150639 - 30s - loss: 1.1600 - acc: 0.6772 - val_loss: 1.1924 - val_acc: 0.6654\n",
      "Epoch 6/1000\n",
      "150639/150639 - 30s - loss: 1.0982 - acc: 0.6905 - val_loss: 1.1809 - val_acc: 0.6681\n",
      "Epoch 7/1000\n",
      "150639/150639 - 30s - loss: 1.0517 - acc: 0.7013 - val_loss: 1.1741 - val_acc: 0.6694\n",
      "Epoch 8/1000\n",
      "150639/150639 - 30s - loss: 1.0120 - acc: 0.7107 - val_loss: 1.1708 - val_acc: 0.6715\n",
      "Epoch 9/1000\n",
      "150639/150639 - 30s - loss: 0.9798 - acc: 0.7166 - val_loss: 1.1714 - val_acc: 0.6730\n",
      "Epoch 10/1000\n",
      "150639/150639 - 31s - loss: 0.9466 - acc: 0.7239 - val_loss: 1.1749 - val_acc: 0.6725\n",
      "Validation accuracy: 0.6724618673324585, loss: 1.1749119563603547\n"
     ]
    }
   ],
   "source": [
    "# 20,000 features of one and two word ngrams\n",
    "hist = train_ngram_model(data, n_gram = (1,2), feature_size = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 20000 single and biword word ngrams gave a validation accuracy of ~67% (72% training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150639 samples, validate on 50214 samples\n",
      "Epoch 1/1000\n",
      "150639/150639 - 30s - loss: 2.3972 - acc: 0.4069 - val_loss: 1.6497 - val_acc: 0.5751\n",
      "Epoch 2/1000\n",
      "150639/150639 - 31s - loss: 1.5460 - acc: 0.5895 - val_loss: 1.3257 - val_acc: 0.6392\n",
      "Epoch 3/1000\n",
      "150639/150639 - 31s - loss: 1.3062 - acc: 0.6407 - val_loss: 1.2204 - val_acc: 0.6588\n",
      "Epoch 4/1000\n",
      "150639/150639 - 32s - loss: 1.1779 - acc: 0.6694 - val_loss: 1.1762 - val_acc: 0.6685\n",
      "Epoch 5/1000\n",
      "150639/150639 - 31s - loss: 1.0884 - acc: 0.6910 - val_loss: 1.1570 - val_acc: 0.6700\n",
      "Epoch 6/1000\n",
      "150639/150639 - 31s - loss: 1.0255 - acc: 0.7058 - val_loss: 1.1494 - val_acc: 0.6719\n",
      "Epoch 7/1000\n",
      "150639/150639 - 31s - loss: 0.9710 - acc: 0.7180 - val_loss: 1.1495 - val_acc: 0.6707\n",
      "Epoch 8/1000\n",
      "150639/150639 - 32s - loss: 0.9231 - acc: 0.7291 - val_loss: 1.1560 - val_acc: 0.6693\n",
      "Validation accuracy: 0.6692556142807007, loss: 1.156042558106963\n"
     ]
    }
   ],
   "source": [
    "# 20,000 features of one word ngrams\n",
    "hist = train_ngram_model(data, n_gram = (1,1), feature_size = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 20000 single word ngrams gave a validation accuracy of ~67% (73% training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
